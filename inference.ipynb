{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import GPTConfig, GPT, ParallelGPT, LinearGPT, ConvGPT\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model_type, text, max_new_tokens, device, temperature=0.1):\n",
    "    if model_type=='gpt': model = GPT(GPTConfig(vocab_size=50304))\n",
    "    elif model_type==\"pgpt\": model = ParallelGPT(GPTConfig(vocab_size=50304))\n",
    "    elif model_type==\"lgpt\": model = LinearGPT(GPTConfig(vocab_size=50304))\n",
    "    elif model_type==\"cgpt\": model = ConvGPT(GPTConfig(vocab_size=50304))\n",
    "    cp = torch.load(f'checkpoints/{model_type}.pt', map_location=device)\n",
    "    model.load_state_dict(cp['model'])\n",
    "    model.to(device)\n",
    "    encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "    tokens = torch.tensor([encoding.encode(text)])\n",
    "    op = model.generate(idx=tokens, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    return encoding.decode(list(op.cpu().numpy())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type: gpt\n",
      "number of parameters: 51.51M\n",
      "I am afraid !!! Do so as the prestige of saintship and fabulous faultlessness lies in their true riches and life, what sorts of mission lessons there are?\n",
      "Host providing our persons with rich, our blessed majesty\n",
      "Commerit serves us well indeed in higher\n",
      "====================================================================================================\n",
      "model_type: pgpt\n",
      "number of parameters: 51.51M\n",
      "I am afraid  having took an oath on March 40 to what is appropriate. 1940 also found that TenbanETS Being Styles from CIL Media Laboratories produces data of over 2,500MB/cm fuel-efficient code into critical operations. Much such use seems to be\n",
      "====================================================================================================\n",
      "model_type: cgpt\n",
      "number of parameters: 51.51M\n",
      "I am afraid  said\" notriet later than beforeàâ; though ___ had written out the correct solution. For everyone – as Ben Afterit was until witzed until anyone in the time,, the conopanting silly heroine advocated no dissent.\n",
      "\n",
      "====================================================================================================\n",
      "model_type: lgpt\n",
      "number of parameters: 51.51M\n",
      "I am afraid  (must [just] sentence no longer have to do her letter!) lozz,\" she said, \"when the Lost Wild Problems - They are afraid Cognher was not speaking to me (you excellent) Stamp stamp, Great James, Via Ilc\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "text = \"I am afraid \"\n",
    "for model_type in ['gpt', 'pgpt', 'cgpt', 'lgpt']:\n",
    "    print(f'model_type: {model_type}')\n",
    "    print(generate(model_type, text, 50, \"cpu\", 1.0))\n",
    "    print('='*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
